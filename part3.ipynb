{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Part 3: Adversarial, Transferability and Robustification\n",
        "\n"
      ],
      "metadata": {
        "id": "TDfewngP4NCC",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recommand you to use Google Colab to edit and run this notebook. You can also install jupyter on your own computer."
      ],
      "metadata": {
        "id": "yfLSN2Lr1spr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6TZpU6QwAYp6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Prepare data\n",
        "\n",
        "You can familiarise yourself with MNIST, a small size dataset, on its Wikipedia article [https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database). MNIST is composed of 28x28 grayscaled images of handwritten digits. This is a classification task with 10 classes (10 digits)."
      ],
      "metadata": {
        "id": "UmdwipDFgh90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading\n",
        "mnist = fetch_openml('mnist_784', as_frame=False, cache=True)\n"
      ],
      "metadata": {
        "id": "IcmjJHdxBDuZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = mnist[\"data\"]\n",
        "y = mnist[\"target\"]"
      ],
      "metadata": {
        "id": "HdRM7qC3KBiI",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data exploration\n",
        "print(f\"Shape of x: {x.shape}\")\n",
        "print(f\"Min, max x: {x.min(), x.max()}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(f\"Classes in y: {np.unique(y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi4EdVlaBy5-",
        "outputId": "03a1c422-0d9a-45b7-90c2-d32a225c10ea",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x: (70000, 784)\n",
            "Min, max x: (0.0, 255.0)\n",
            "Shape of y: (70000,)\n",
            "Classes in y: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "x = torch.from_numpy(x.astype(float)).float()\n",
        "y = torch.from_numpy(y.astype(int)).type(torch.LongTensor)\n",
        "# Shape\n",
        "x = x.reshape(-1, 1, 28, 28)\n",
        "# Scaler\n",
        "x = (x - x.min()) / (x.max() - x.min())\n"
      ],
      "metadata": {
        "id": "GCUKJb8fCXvx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train, shuffle=True)"
      ],
      "metadata": {
        "id": "LBSIna-OEnq9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Adversarial examples\n",
        "\n",
        "The goal of this first part is to generate adversarial examples on a simple dataset called MNIST. MNIST is a dataset of 28x28 black and white images that represents hand-written digits, and their associate label 0,1,...,9.\n",
        "\n",
        "You can use the following ressource to help you [https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#).\n"
      ],
      "metadata": {
        "id": "wkcToEG85CCC",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train a Neural Network using the PyTorch library.\n",
        "\n",
        "The architecture of the models and the training hyper-parameters are given below.\n",
        "We recommend using these parameters, the SGD optimizer and the Cross Entropy loss.\n"
      ],
      "metadata": {
        "id": "9unhURg05qcR",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OAsZAMTvMKPX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "momentum=0.9\n",
        "epochs = 10\n",
        "batch_size = 64\n"
      ],
      "metadata": {
        "id": "gTG7NSpuiI6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: use SGD with the provided hyperparameters\n",
        "model_0 = None\n",
        "optimizer = None"
      ],
      "metadata": {
        "id": "I5Ogq-UIjMWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: use CrossEntropyLoss.\n",
        "loss_func = None"
      ],
      "metadata": {
        "id": "R-jf6XNgjDFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size/batch_size)):\n",
        "\n",
        "\n",
        "        # Compute prediction and loss\n",
        "\n",
        "\n",
        "        ## YOUR CODE HERE:\n",
        "\n",
        "        # Backpropagation\n",
        "\n",
        "        ## YOUR CODE HERE:\n",
        "\n"
      ],
      "metadata": {
        "id": "2MujJh_5jonb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GIVEN, to evaluate the progress of the training at each epoch\n",
        "def val_loop(dataloader, model, loss_fn, epoch_i):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Epoch {epoch_i}, Val Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")"
      ],
      "metadata": {
        "id": "9lwTxQ60kaXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, x_train, y_train, x_val, y_val, optimizer, batch_size, loss_func, epochs):\n",
        "    # Data processing\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "    val_dataset = TensorDataset(x_val, y_val)\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=2000,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Main train loop\n",
        "    ## YOUR CODE HERE:\n"
      ],
      "metadata": {
        "id": "hn6HM3Y7HP6E",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: train the model using the training function you just implemented.\n"
      ],
      "metadata": {
        "id": "zDzi2wGpk-nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluate clean accuracy of the Neural Network using a test set that has not been used for training."
      ],
      "metadata": {
        "id": "ggr_3NzH6faf",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model into evaluation mode\n",
        "model_0.eval()"
      ],
      "metadata": {
        "id": "Iyr4Zuo7Gsqz",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: Evaluate model accuracy\n",
        "\n",
        "accuracy = None\n",
        "print(f\"Clean accuracy of the model is {accuracy}.\")"
      ],
      "metadata": {
        "id": "JItRLnejlXzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement and execute the PGD attack on 1000 examples of the testing set. The hyperparameters of PGD are given below.\n",
        "The perturbation is bounded by a maximum L-infinity norm, called epsilon (eps), which means that each pixel can be perturbed between -eps and +eps. We initialy set the maximum perturbation to eps = 32/255. For simplicity, you can set the step size alpha = epsilon / 10, and run PGD with only one random restart.\n",
        "\n",
        "You can find the description of PGD in the paper [https://arxiv.org/abs/1706.06083](https://arxiv.org/abs/1706.06083) and an example of another adversarial attack on the PyTorch documentation [https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html).\n",
        "Tips: use the F.cross_entropy loss during the attack.\n"
      ],
      "metadata": {
        "id": "lSkQn-RV6lYF",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 1000\n",
        "eps = 32/255\n",
        "n_iter = 50\n",
        "alpha = eps / 10\n"
      ],
      "metadata": {
        "id": "d7BKUQW6tExt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: Generate adversarial examples\n"
      ],
      "metadata": {
        "id": "o1fT3GOLn6Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Show the robust accuracy of model_0, that is the accuracy of the model on the adversarial examples."
      ],
      "metadata": {
        "id": "U0O9f_OSnfNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: Evaluate model robust accuracy\n"
      ],
      "metadata": {
        "id": "chHBAjRin7ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Show the impact of the maximum perturbation allowed (denoted epsilon)."
      ],
      "metadata": {
        "id": "GzvYoy6kAWCE",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = [8/255, 16/255, 32/255, 64/255]\n",
        "alpha = [e/10 for e in eps]"
      ],
      "metadata": {
        "id": "lJv7ipEEnyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE: compute the adversarial examples for each provided epsilon\n",
        "## (maximum l-infinity norm of the perturbation), and compute the associated robust accuracy\n",
        "## Use a graph to display your result. You may use the [Matplotlib] (https://matplotlib.org/stable/index.html)."
      ],
      "metadata": {
        "id": "pIsJzfm2oJsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Using matplotlib, plot 10 adversarial examples, along with their corresponding original images. Choose one original image classified per class (the 10 class should be represented). For each image (adversarial and original), add on the plot the predicted class of the image.\n"
      ],
      "metadata": {
        "id": "BhIK7v5vqJWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "1cDtwHhRqHLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Please comment your results of this section."
      ],
      "metadata": {
        "id": "lJ2b1k-uAnNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER HERE**"
      ],
      "metadata": {
        "id": "L5hkNyHhA2kW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Transferability"
      ],
      "metadata": {
        "id": "RIw9leNrGeXW",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will see how adversarial examples generated on one model can be adversarial on another model using a different architecture.\n",
        "Let suppose a second model which parameters are unknown. For instance, it could be a model deploy on a cloud platform. We will use the examples generated in Section 1 on model_0 to fool this new model denoted model_1.\n",
        "We say that model_0 is a surrogate for model_1."
      ],
      "metadata": {
        "id": "YzMOFr6gG9NZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define a neural network architecture for MNIST different than the one used in Section 1."
      ],
      "metadata": {
        "id": "Oc5SBJRMI7s5",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## GIVEN\n",
        "class FullyConnectedNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Yyk8EdWJHV8Y",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Train the neural network model_1 with the same hyperparameters as model_0\n"
      ],
      "metadata": {
        "id": "roIKZhH_J5uJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE\n",
        "model_1 = None\n",
        "optimizer = None  # create a new optimizer object when you train a new model"
      ],
      "metadata": {
        "id": "jxtPU9qDra7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the ratio of successful adversarial examples on model_0 that transfers to model_1 (ie. that are also adversarial for model_1)?\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jL6wbtlgf_bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.eval()\n",
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "cCCiLpJzsDSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you conclude about the robustness of the model? Can [secrecy](https://en.wikipedia.org/wiki/Security_through_obscurity) defend a model?"
      ],
      "metadata": {
        "id": "ILXDrrMU_-OO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER HERE**"
      ],
      "metadata": {
        "id": "w_BJuQISAX3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Use adversarial training to robustify the model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gm7YiINCf_bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial training is a common method to robustify models to adversarial examples as described in this paper [https://arxiv.org/abs/1706.06083](https://arxiv.org/abs/1706.06083). In this section you should update the training loop such that 3/4 of the batch is used for training while the remaining forth is first perturbed with PGD and then used for training. You can limit the number of iterations of PDG to 10. Use model_0 architecture from Section 1 in this section."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "umy74k6mf_bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train model_robust using adversarial training. You may want to run it for additional epoch (x2) to reach a similar clean accuracy."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TfC1zwKlf_bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 10  # less iterations to accelerate training. But once trained, we will still evaluate the robust accuracy on more iterations for a more powerful attack.\n",
        "eps = 32/255\n",
        "alpha = eps / 5\n",
        "model_robust = Net()  # newly initialized NN"
      ],
      "metadata": {
        "id": "zBzdZhkstpRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
        "    size = len(dataloader.dataset)\n",
        "    adv_size = int(batch_size/4)\n",
        "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size/batch_size)):\n",
        "\n",
        "        # Generate adversarial examples for a forth of the data\n",
        "\n",
        "        model.eval()\n",
        "        ## YOUR CODE HERE\n",
        "        model.train()\n",
        "\n",
        "        # Compute prediction and loss\n",
        "\n",
        "        ## YOUR CODE HERE:\n",
        "\n",
        "        # Backpropagation\n",
        "\n",
        "        ## YOUR CODE HERE:\n",
        "\n"
      ],
      "metadata": {
        "id": "xeTu-9edtlFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "## YOUR CODE HERE: The rest of training implementation is unchanged.\n",
        "## Do not reuse the same optimizer object!!!"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xA1T6hFyf_bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compare the robust accuracies of model_0 and model_robust using the same hyperparameters of PGD for different eps size, use a graph to show your results."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0tD5Q08Xf_by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 1000\n",
        "n_iter = 50\n",
        "eps = [8/255, 16/255, 32/255, 64/255]\n",
        "alpha = [e/10 for e in eps]"
      ],
      "metadata": {
        "id": "YxCK8_1quznJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "FvIbjTPmvFUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**: Please comment your results. Does adversarial training appears to be a valid defense? Please develop threads to validity of the robust accuracy evaluation carried out here. What could be done to improve the evaluation of the robustness of the model?"
      ],
      "metadata": {
        "id": "WC7D9bT3A5Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER HERE**"
      ],
      "metadata": {
        "id": "xT5WD9AZBfVE"
      }
    }
  ]
}